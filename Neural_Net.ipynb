{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Venue ID</th>\n",
       "      <th>Venue category ID</th>\n",
       "      <th>Venue category name</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Timezone offset in minutes</th>\n",
       "      <th>UTC time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>49bbd6c0f964a520f4531fe3</td>\n",
       "      <td>4bf58dd8d48988d127951735</td>\n",
       "      <td>Arts &amp; Crafts Store</td>\n",
       "      <td>40.719810</td>\n",
       "      <td>-74.002581</td>\n",
       "      <td>-240</td>\n",
       "      <td>Tue Apr 03 18:00:09 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>979</td>\n",
       "      <td>4a43c0aef964a520c6a61fe3</td>\n",
       "      <td>4bf58dd8d48988d1df941735</td>\n",
       "      <td>Bridge</td>\n",
       "      <td>40.606800</td>\n",
       "      <td>-74.044170</td>\n",
       "      <td>-240</td>\n",
       "      <td>Tue Apr 03 18:00:25 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>4c5cc7b485a1e21e00d35711</td>\n",
       "      <td>4bf58dd8d48988d103941735</td>\n",
       "      <td>Home (private)</td>\n",
       "      <td>40.716162</td>\n",
       "      <td>-73.883070</td>\n",
       "      <td>-240</td>\n",
       "      <td>Tue Apr 03 18:02:24 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>395</td>\n",
       "      <td>4bc7086715a7ef3bef9878da</td>\n",
       "      <td>4bf58dd8d48988d104941735</td>\n",
       "      <td>Medical Center</td>\n",
       "      <td>40.745164</td>\n",
       "      <td>-73.982519</td>\n",
       "      <td>-240</td>\n",
       "      <td>Tue Apr 03 18:02:41 +0000 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87</td>\n",
       "      <td>4cf2c5321d18a143951b5cec</td>\n",
       "      <td>4bf58dd8d48988d1cb941735</td>\n",
       "      <td>Food Truck</td>\n",
       "      <td>40.740104</td>\n",
       "      <td>-73.989658</td>\n",
       "      <td>-240</td>\n",
       "      <td>Tue Apr 03 18:03:00 +0000 2012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User ID                  Venue ID         Venue category ID  \\\n",
       "0     470  49bbd6c0f964a520f4531fe3  4bf58dd8d48988d127951735   \n",
       "1     979  4a43c0aef964a520c6a61fe3  4bf58dd8d48988d1df941735   \n",
       "2      69  4c5cc7b485a1e21e00d35711  4bf58dd8d48988d103941735   \n",
       "3     395  4bc7086715a7ef3bef9878da  4bf58dd8d48988d104941735   \n",
       "4      87  4cf2c5321d18a143951b5cec  4bf58dd8d48988d1cb941735   \n",
       "\n",
       "   Venue category name   Latitude  Longitude  Timezone offset in minutes  \\\n",
       "0  Arts & Crafts Store  40.719810 -74.002581                        -240   \n",
       "1               Bridge  40.606800 -74.044170                        -240   \n",
       "2       Home (private)  40.716162 -73.883070                        -240   \n",
       "3       Medical Center  40.745164 -73.982519                        -240   \n",
       "4           Food Truck  40.740104 -73.989658                        -240   \n",
       "\n",
       "                         UTC time  \n",
       "0  Tue Apr 03 18:00:09 +0000 2012  \n",
       "1  Tue Apr 03 18:00:25 +0000 2012  \n",
       "2  Tue Apr 03 18:02:24 +0000 2012  \n",
       "3  Tue Apr 03 18:02:41 +0000 2012  \n",
       "4  Tue Apr 03 18:03:00 +0000 2012  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import powerlaw\n",
    "from sklearn.metrics import silhouette_score\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from tslearn.metrics import dtw\n",
    "from tslearn.clustering import KShape\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "import random\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.lines as mlines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "rc('font',**{'family':'serif','serif':['Times New Roman']})\n",
    "rc('text', usetex=True)\n",
    "%matplotlib inline\n",
    "#Powerlaw: A Python Package for Analysis of Heavy-Tailed Distributions\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Open the text file for reading with the specified encoding\n",
    "with open(r'C:\\Users\\oscar\\OneDrive - Universidad Nacional de Colombia\\Tesis\\Comportamineto humano\\Check-in-NY\\dataset_tsmc2014\\dataset_TSMC2014_NYC.txt', 'r', encoding='latin-1') as file:\n",
    "    # Read each line in the file\n",
    "    for line in file:\n",
    "        # Split the line into columns based on the tab delimiter\n",
    "        columns = line.strip().split('\\t')\n",
    "        \n",
    "        # Append the columns to the data list\n",
    "        data.append(columns)\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['User ID', 'Venue ID', 'Venue category ID', 'Venue category name', 'Latitude', 'Longitude', 'Timezone offset in minutes', 'UTC time']\n",
    "\n",
    "# Create a DataFrame from the data list\n",
    "df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "# Optionally, convert columns to appropriate data types\n",
    "df['Latitude'] = df['Latitude'].astype(float)\n",
    "df['Longitude'] = df['Longitude'].astype(float)\n",
    "df['Timezone offset in minutes'] = df['Timezone offset in minutes'].astype(int)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`start_index+length=100 > end_index=99` is disallowed, as no part of the sequence would be left to be used as current step.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m group \u001b[38;5;241m=\u001b[39m group\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTC time\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Ordenar por tiempo\u001b[39;00m\n\u001b[0;32m     22\u001b[0m features \u001b[38;5;241m=\u001b[39m group[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLongitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHour\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDayOfWeek\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m---> 23\u001b[0m generator \u001b[38;5;241m=\u001b[39m \u001b[43mTimeseriesGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(generator)):\n\u001b[0;32m     25\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m generator[i]\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\all-in-purpose\\Lib\\site-packages\\keras\\src\\preprocessing\\sequence.py:158\u001b[0m, in \u001b[0;36mTimeseriesGenerator.__init__\u001b[1;34m(self, data, targets, length, sampling_rate, stride, start_index, end_index, shuffle, reverse, batch_size)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_index:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`start_index+length=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m > end_index=\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis disallowed, as no part of the sequence \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwould be left to be used as current step.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_index, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_index)\n\u001b[0;32m    163\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `start_index+length=100 > end_index=99` is disallowed, as no part of the sequence would be left to be used as current step."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Normalización de latitud y longitud\n",
    "scaler = MinMaxScaler()\n",
    "df[['Latitude', 'Longitude']] = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "\n",
    "# Especificación del formato de la marca de tiempo\n",
    "date_format = \"%a %b %d %H:%M:%S +0000 %Y\"\n",
    "\n",
    "# Conversión de la marca de tiempo a características útiles\n",
    "df['UTC time'] = pd.to_datetime(df['UTC time'], format=date_format)\n",
    "df['Hour'] = df['UTC time'].dt.hour\n",
    "df['DayOfWeek'] = df['UTC time'].dt.dayofweek\n",
    "\n",
    "# Agrupación por usuario\n",
    "grouped = df.groupby('User ID')\n",
    "\n",
    "# Definición de la secuencia de longitud n\n",
    "sequence_length = 10\n",
    "data = []\n",
    "\n",
    "for user_id, group in grouped:\n",
    "    group = group.sort_values(by='UTC time')  # Ordenar por tiempo\n",
    "    features = group[['Latitude', 'Longitude', 'Hour', 'DayOfWeek']].values\n",
    "    generator = TimeseriesGenerator(features, features, length=sequence_length, batch_size=1)\n",
    "    for i in range(len(generator)):\n",
    "        x, y = generator[i]\n",
    "        data.append((x[0], y[0]))  # Asegurarse de extraer correctamente los datos\n",
    "\n",
    "# Verificación de las dimensiones de los datos\n",
    "print(f\"Número de secuencias: {len(data)}\")\n",
    "print(f\"Dimensiones de cada secuencia: {data[0][0].shape if data else 'N/A'}\")\n",
    "print(f\"Dimensiones de cada etiqueta: {data[0][1].shape if data else 'N/A'}\")\n",
    "\n",
    "# Convertir la lista de secuencias a numpy arrays y asegurar la forma correcta\n",
    "X = np.array([item[0] for item in data])\n",
    "y = np.array([item[1] for item in data])\n",
    "\n",
    "# Ajustar las dimensiones para que sean compatibles con el modelo LSTM\n",
    "X = X.reshape(len(X), sequence_length, -1)  # Debería ser (n_samples, sequence_length, n_features)\n",
    "y = y.reshape(len(y), -1)  # Debería ser (n_samples, n_features)\n",
    "\n",
    "# Verificación de las formas de las entradas\n",
    "print(\"Forma de X:\", X.shape)  # Debería ser (n_samples, sequence_length, n_features)\n",
    "print(\"Forma de y:\", y.shape)  # Debería ser (n_samples, n_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\oscar\\anaconda3\\envs\\all-in-purpose\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "6769/6769 [==============================] - 70s 10ms/step - loss: 12.1782\n",
      "Epoch 2/10\n",
      "6769/6769 [==============================] - 72s 11ms/step - loss: 11.7502\n",
      "Epoch 3/10\n",
      "6769/6769 [==============================] - 71s 10ms/step - loss: 11.6577\n",
      "Epoch 4/10\n",
      "6769/6769 [==============================] - 76s 11ms/step - loss: 11.6087\n",
      "Epoch 5/10\n",
      "6769/6769 [==============================] - 86s 13ms/step - loss: 11.5682\n",
      "Epoch 6/10\n",
      "6769/6769 [==============================] - 78s 12ms/step - loss: 11.5433\n",
      "Epoch 7/10\n",
      "6769/6769 [==============================] - 78s 11ms/step - loss: 11.5205\n",
      "Epoch 8/10\n",
      "6769/6769 [==============================] - 78s 12ms/step - loss: 11.5093\n",
      "Epoch 9/10\n",
      "6769/6769 [==============================] - 83s 12ms/step - loss: 11.4898\n",
      "Epoch 10/10\n",
      "6769/6769 [==============================] - 76s 11ms/step - loss: 13.2321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x22e86c94e90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creación del modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(sequence_length, X.shape[2])))\n",
    "model.add(Dense(X.shape[2]))  # Salida con el mismo número de características que la entrada\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model.fit(X, y, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6769/6769 [==============================] - 48s 7ms/step\n",
      "Anomalies: [ True  True  True ...  True  True  True]\n",
      "MSE: [13.43465706 17.11985166 16.90360602 ... 11.06232316 24.14922832\n",
      " 20.01994358]\n"
     ]
    }
   ],
   "source": [
    "def detect_anomalies(model, X, threshold=0.01):\n",
    "    predictions = model.predict(X)\n",
    "    mse = np.mean(np.power(X - np.expand_dims(predictions, axis=1), 2), axis=(1, 2))  # Ajuste en el cálculo del MSE\n",
    "    anomalies = mse > threshold\n",
    "    return anomalies, mse\n",
    "\n",
    "\n",
    "# Detectar anomalías\n",
    "anomalies, mse = detect_anomalies(model, X)\n",
    "\n",
    "# Imprimir resultado\n",
    "print(\"Anomalies:\", anomalies)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of True values (anomalies): 216598\n",
      "Number of False values (non-anomalies): 0\n"
     ]
    }
   ],
   "source": [
    "true_count = np.count_nonzero(anomalies)\n",
    "false_count = len(anomalies) - true_count\n",
    "\n",
    "print(\"Number of True values (anomalies):\", true_count)\n",
    "print(\"Number of False values (non-anomalies):\", false_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column_name'] = df['column_name'].str.replace(r'^(0550|0570)', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by latitude and longitude\n",
    "grouped = df.groupby(['Latitude', 'Longitude'])\n",
    "\n",
    "# Count the number of occurrences and create a list of IDs\n",
    "count = grouped.size().reset_index(name='Count')\n",
    "ids = grouped['ID'].apply(list).reset_index(name='IDs')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all-in-purpose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
